\chapter{Discussion}\label{chapter:discussion}
This chapter will discuss the experimental results compiled in the previous chapter by closely following the progression of those results. It will then conclude this thesis by reviewing its contribution and giving an outlook on open research questions within the topic.
\section{Results}
\subsubsection{Hyperparameters and Training}
As it turns out, training a matrix capsule network is extremely difficult, far more so than implementing any of the mechanics, including the EM routing algorithm. This is mostly because it is very hard to find a stable tuple of hyperparameters, let alone a good one. As is shown in the visualization of the accuracy over the hyperparameters, they are almost thermally distributed. This is even more apparent when compared directly to the other network architectures’ hyperparameters. While experience and maturity of the technology will play a role in this, vector capsule networks on the other hand do not suffer this problem. On the contrary, their hyperparameters are strongly correlated, making it easy to find at least stable local minima. One is therefore left with the choice of performing an extensive hyperparameter search or come up with some clever scheme to find them more quickly (e.g. genetic algorithms, swarm optimization etc.).  Once hyperparameters are found however, training is rather straightforward. Some care has to be taken not to cancel the training too early. Both kinds of capsules will often fluctuate for the first few epochs, learning little to nothing, before they will suddenly make rapid progress. One reason for this behavior could be their \emph{bootstraping} process. Before the routing mechanism can take over the propagation of signals between capsule layers, the activities and initial poses of the first capsule layer have to be initialized. This is done by a discriminatively learned standard convolutional layer. It is reasonable to assume that capsule layers will only begin to learn once this bootstrapping is sufficiently advanced.

This is in stark contrast to the spiking neural network that is used. The SNN is remarkably stable over a large configuration of hyperparameters. In fact, it is difficult to find hyperparameters that will not allow the SNN to learn at least a little about the data. However, similarly to the capsule networks, the rate coded SNN will also fluctuate considerably during training. Additionally, the SNN will usually converge to a lower performing state. Ideally, the training of similar spiking networks should therefore include some kind of early stopping \cite{ prechelt1998automatic}.\noindent

These observations are made with CNNs as an established baseline. Little has to be said about training and optimizing standard convolutional neural networks. The amount of research published makes it possible to find recommendations regarding myriads of application domains and data formats. Also, the availability of streamlined software libraries simplifies development and optimizations 
\subsubsection{Runtime}
If only the degrees of freedom (i.e. trainable parameters) are considered, matrix capsules constitute the best hypothesis as to how the visual system parses an image and represents an object recognized within that image. This claim is substantiated by the fact that in these experiments a matrix capsule network with only $\num{67442}$ parameters achieves similar classification performance to optimized networks with several orders of magnitude more degrees of freedom. And according to the minimum description length principle, the hypothesis that compresses the data most, is the most viable \cite{ rissanen1978modeling}. However, this very sparse representation comes at very high computational costs and memory requirements. A matrix capsule network quickly takes up several $\si{\giga\byte}$ of memory compared to tens of $\si{\mega\byte}$ for a CNN with a few million degrees of freedom. The same goes for training speed and inference latency. Matrix capsules are very slow compared to CNNs or any of the other networks, especially regarding training. Essentially, even though they offer a huge representational advantage, matrix capsules are at the time of writing unable to benefit from this according to all other metrics applied.

The spiking neural network is the only one that comes close to the matrix capsule’s representation with $\num{103005}$ parameters. This is especially impressive, considering that network’s extremely simple topology. It consists of only one fully connected hidden layer with 100 neurons. But compared to the matrix capsule network that representational advantage is not as costly. Quite the opposite is true, the SNN requires the least amount of memory and still runs a lot faster than matrix capsule networks. However, due to its asynchronous spiking nature, inference takes almost as long as training and mini batches cannot be processed in parallel. The later is particularly disadvantageous in inference. Even if a network prefers small batch sizes during training as the vector capsules clearly do, such a preference is irrelevant during testing as there are no gradients required for inference and arbitrarily sized mini batches may be processed – fully leveraging the might of highly parallel computing on GPUs. While each additional sample in a mini batch will only require little additional space in memory ($\ll \num{100}$MB), high memory usage will limit the possible size of mini batches. For practical applications runtime resource requirements are more important than the sparsity of internal representation. As long as there is no widespread use of neuromorphic hardware to leverage sparsity into energy efficiency CNNs will probably remain the most attractive option. Vector capsule networks however fall somewhere in-between. Their memory footprint is not quite as big as that of the matrix capsules and they run a lot faster, but their representation also is nowhere near as sparse. The reason for the capsule networks’ memory footprint is also the reason responsible for their relatively sparse representation, the routing algorithm. By dynamically routing the activities between capsules by agreement about pose, a prior about the structure of features is introduced that allows to encode the features using a shorter description length. However, the part-whole poses are learned discriminatively using stochastic gradient descent with backpropagation. This includes computing the gradient over the iterative routing algorithm, i.e. the already quite complex computational graph of the routing algorithm’s output becomes an input node into the same algorithm again and grows exponentially with each iteration. This makes it difficult to train capsule networks, especially matrix capsules with their very complicated EM routing but also requires a lot of memory and computational power.
\subsubsection{Object Recognition}
As the object recognition dataset is entirely new, it is difficult to assess the performance in absolute numbers. However, it is clear that the networks are not able to fully solve the task. This is by no means a bad thing, as a good benchmark needs to be difficult enough to allow discriminating the evaluated methods. It does come somewhat as a surprise that the vector capsules achieve the highest classification accuracy, as Hinton et al. reported that their matrix capsules outperformed the vector capsules introduced by Sabour et al. in all regards. However, Hinton et al. also reported achieving the best results with 3 iterations of EM routing. In these experiments, no stable hyperparameter tuple allowing for 3 EM successful routing iterations are found, despite extensive hyperparamer searches. Nevertheless, matrix capsules come in second, followed by the CNN with the SNN coming in last by a large margin. The SNN’s performance is to be expected, as SNNs generally still lack behind second generation neural networks in classification tasks like object recognition.

There appear to be certain object classes in the dataset that are more easily to classify. The cup and cutlery is almost perfectly classified by all the networks. These classes also happen to be largest and smallest by pixel area, so it is probable that they can be identified by simple statistical properties of the pixels. All of the networks except the SNN have problems keeping the chess pieces apart from the lighters. This could be due to the fact that these two classes have both a similar pixel area and a right degree of symmetry (most lighters have 1 symmetry axis and the many chess pieces have rotational symmetry). Why the SNN in particular is not susceptible to this mistake is not quite clear but it is evened out by other misclassifications specific to the SNN that see1m to make much less sense, like mistaking lighters or chess pieces for chairs, even though their area and morphology is very much different. There isn’t too much variation between the behavior of the two capsule networks and the CNN. There is however a marked difference between the SNN and the other networks. This may very well be due to the fact that the other networks all are second generation networks with convolutional layers, while the SNN is a third generation network with only a fully connected layer. As such the two groups may focus on different features in the images.\newpage
\vspace{.5cm}\subsubsection{Generalization}
The results of the alternate lighting setups suggest that the matrix capsule network may be the one with the best ability to generalize. However, these are only 3 point estimates and if the distribution and depending on what the true distribution of perturbation looks like, these measurements may not be statistically significant. This approach is representative of how generalization in the presence of perturbation is often evaluated.

The first real test of generalization according to the introduced concept of robustness as conditional accuracy distribution is robustness against occlusion. The result shows a nice correlation with the coverage as one might expect. It is interesting to note however, that the curves to occasionally cross each other. This is an important clue, because for point estimates this means depending on where they would sample this distribution different conclusions would be drawn about the networks’ ability to generalize. With the full distribution across perturbation space this does not happen and the ability to generalize can still be summarized into a single scalar value by calculating the AUC. In this case the SNN clearly is the most robust classifier, only for the first $\SI{20}{\percent}$ of occlusion is the vector capsule better and generalizing. If only samples with occlusion up to $\SI{20}{\percent}$ were available (i.e. not the full perturbation distribution) the networks’ ability to generalize would be misjudged.

Next is the robustness against part-whole decomposition. This particular test is chosen because capsule networks and matrix capsules in particular are designed to learn part-whole relationships. If those relationships are deconstructed the routing should no longer function as the votes won’t be in agreement anymore. Also, the capsule networks are motivated by the fact that they retain precise information on the location of features compared to CNNs that presumably lose this information in pooling operations. The result from the robustness test however, does not support these assumptions. Curiously the capsule networks, particularly the matrix capsules perform the worst (i.e. they still classify the objects well even though part-whole relationships are dissolved). Whereas the CNN and SNN break down quickly and already at a permutation of $5\cross 5$ tiles no longer recognize the objects. Generally, as the number of tiles goes up and their respective size goes down the ability to recognize the objects goes down (robustness against part-whole decomposition goes up) as expected. One possible explanation is that the capsule networks learned more about the statistical properties of pixels than any complicated feature relationships. This would allow them to still classify images even if they are no longer coherent while ignoring part-whole relationships.

The final test is also inspired by the motivation behind capsule networks. Matrix capsules are designed as shape recognition classifiers that are supposed to ideally learn the pose between features of adjacent capsule layers. In this pose space the complex nonlinear transformation of pixel intensities that are observed with a change in pose become simple linear transformations. In theory this would make capsule networks particularly suited to generalize to novel viewpoints. Again, this cannot be confirmed. In the results, although there is a lot of fluctuation and variance no single network is able to set itself apart by a large margin. What can be observed however, is that the matrix capsules show the least amount of fluctuation in the presence of incrementally novel viewpoints and generalize slightly better than the vector capsules and the CNN. But in this test too, the SNN performs surprisingly well. A possible explanation for the robustness of the SNN could lie in its dynamic nature. The input is generated by randomly sampling from a distribution who's firing rate is proportional to the respective pixel's intensity. Because of this stochasticity it's possible that if a perturbation is present it may not afflict all the sampled spikes fully. As long as there are useful spikes propagating within the network they will contribute to the error signal which is approximated by summing all the spikes of the output neurons. In other words the SNN already learns a noisy representation of the data due to the way the input signal is generated (acting like a form of data augmentation) and therefore generalizes well to added perturbations.
\section{Conclusion}
Within the framework of this thesis a new synthetic and variable dataset on basis of the Neurorobotics Platform is developed that can be fine-tuned to evaluate classifiers. In particular perturbations are introduced to gauge a neural network’s ability to generalize. It is shown that the ability to parameterize a perturbation and sample it across the entire space of perturbations is important to make a sound judgment about a network’s ability to generalize. More specifically the ideas and methods developed are used to evaluate the performance of neural networks as well as their ability to generalize in computer vision tasks. Capsule networks are chosen due to their claim to increased biological plausibility over traditional convolutional neural networks and motivation as being especially well at generalizing. As additional baselines, CNNs are included as the established approach in machine learning and spiking neural networks because of their biological plausibility. Eventually, even though they perform very well at object recognition, capsule networks cannot deliver on their promise of increased generalization. Interestingly a simple spiking neural network, which was not specifically designed to be particularly well at generalizing, performed best at generalizing in the presence of perturbations. In addition, the architecture and motivation behind capsule networks is derived in detail and many insights into operating them are given. At this point in their development the additional computational costs and difficulty in training them means that networks based on matrix capsules are not a viable alternative to CNNs. Vector capsules do not suffer as much from these drawbacks and perform very well at object recognition. They could conceivably be used instead of CNNs in certain areas. Regarding biological plausibility, matrix capsules demonstrate that their prior assumptions are a better hypothesis for the coding of objects in visual information as they learn to solve object recognition tasks with only very short description length (i.e. degrees of freedom or parameters). This ability however is contrasted by their need for extreme computational resources and general lack of latency when compared to the other networks. Many of the problems with matrix capsules stem from the choice to train them discriminatively, computing a gradient over the complicated EM algorithm is simply unreasonable. Matrix capsule networks forfeit much of their extensive biological plausibility the moment they are trained by stochastic gradient descent with backpropagation. In the end capsule networks fall in somewhere between the more efficient and reliable CNNs and SNNs with their vast biological fidelity.
\section{Outlook}
There are several ways the work presented in this thesis can be expanded on. Using the Neurorobotics Platform vastly more complex data sets could be synthesized, like animated scenes for semantic classification for example. Or extended to other modalities like spike trains from dynamic vision sensors or point clouds. Given integration of appropriate toolboxes into the platform, vision systems may be trained and evaluated directly within the experiment. As the platform already features a web interface such a development would allow for easy evaluation (web upload) and certification on standardized benchmarks like the conditional accuracy tests introduced here (like an IQ test for artificial intelligence).

Concerning the capsule networks one route forward may be to train them in a biologically inspired fashion such that computing the gradient across the dynamic routing function becomes unnecessary or at least less cumbersome \cite{ bartunov2018assessing}. Most biologically plausible learning algorithms however are designed for spiking neural networks and based on some form of Hebbian learning rule. One obvious approach would be to combine the two methods to reap the benefits of both. The demonstrated robustness and availability of unsupervised biological learning algorithms of SNNs combined with the very good classification results and sparse internal representation of matrix capsules. A first step towards this fusion could be to implement the capsules as groups of spiking neurons and apply linear transformations between the states of those capsules (the part-whole transformation matrices). In a second step a dynamic routing process would have to be developed. This part is a little trickier as spiking neurons fire asynchronously and would not cast their vote all at once during a forward pass as is the case in capsule networks. A higher level capsule could e.g. query its receptive field for their votes once it reaches a certain threshold and based on agreement decide on whether to propagate the signal or not. This is not unlike the idea of inhibition of neural circuits as explored in neuroscience.